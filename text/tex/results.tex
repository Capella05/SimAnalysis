\section{The lens modelling challenge} \label{sec:results}

\subsection{\sw} \label{sec:SpaceWarps}
some words about \sw

\subsection{The simulated lenses} \label{sec:sims}

In order to estimate users performance, \sw included some simulated lensing configurations in their image set.

There are three kinds of simulated lenses.

Sims produced using {\tt gravlens}
\citep{2001astro.ph..2341K,2001astro.ph..2340K}.

\begin{itemize}
  \item {\em Quasar:\/} A singular elliptical isothermal plus constant
    external shear, with a circular Gaussian source.
  \item {\em Galaxy:\/} Lens as with quasar, but elliptical de
    Vaucouleurs source.
  \item {\em Galaxy:\/} Lens is circular NFW plus one dominant
    elliptical SIE and perturbating elliptical SIEs, source as with
    galaxy.
\end{itemize}

Singular isothermal is in equations (33-35) of
\cite{2001astro.ph..2341K} with core radius set to zero. NFW is
equations (48,50) of the same paper, and shear is the $\gamma$ term in
equation (76) of that work.

\subsection{The Test setup} \label{sec:testsetup}
We made use of this simulated lenses and further tested the volunteers abilities to model those simulated lensing images.

The testing was done with a small number of volunteers ($N_v=4$) and a set of simulated configurations ($N_{sim}=29$) chosen to represent some typical configurations.

Two tests were done on the models created by the volunteers:

\begin{enumerate}
  \item Correct identification of extremal points
  \item Reproduction of the mass distribution of the lens
\end{enumerate}

The first test required the volunteers to accomplish 2 tasks: First to identify and distinguish lensed images from background light sources.
Second to come up with a logical ordering of the arrival time of those lensed images.
It also tries to identify the problems volunteers encounter during modelling.

The second test compared the enclosed mass $\kappa(r)$ of the models and the simulations.


\subsection{First test} \label{sec:results.1}

The evaluation of the volunteers performance was done manually / by hand/eye, comparing their input to the actual contour lines.
The actual contour lines were generated from the original simulation parameters.

For each model a set of binary tests was done:

\begin{enumerate}
  \item (approxPlace) have the images approximately been identified correctly (approx within of 10\% of img width)
%  \item (rightPlace) are the images placed precisely (approx. within 5\% of image width)
  \item (rightOrder) is the ordering of the images correct (sad / min / max detection and ordering)
\end{enumerate}

Additionally, ten categories of error that could happend were recorded, where each model could contain multiple of those:

\begin{enumerate}
  \item inaccurate placement in an extended arc
  \item wrongly identified sad and min in 3 image configuration
  \item identified only 3 instead of 5 images
  \item tried to model an arc with a min instead of min-sad-min
  \item PI-err (rotation by 180 degrees; in 5 image configuration, exchanged the ordering of the two saddle points)
  \item PI/2-err (rotation by 90 degree; sad$\longmapsto$min$\longmapsto$sad$\longmapsto$min$\longmapsto$sad)
  \item missed faint image(s)
  \item tried to model an arc with min-sad-min instead of only min
  \item did identify two close by images as one
  \item used 7 or more image to model a 5 image system.
\end{enumerate}

\input{tab/auto/_stats}

By looking at the numbers in table \ref{tab:stats}, we conclude that the volunteers are performing very well identifying images and putting them approximately at the right spot (test 1), with a performance of 92%.
Most of the problems where due to unclear arc-like structures. (Error 1, p=0.18)

\todo{add more detail?} Add more details like total amount of images detected / tot am images (rightPlace fraction)??

The ordering and assignment of minima / maxima / saddlepoint posed a more difficult task.
In p=59\% (N=70) of the cases the volunteers succeeded to identify the right configuration.
Most of the failures are due to error type 6 (PI/2), with N=38, p=32\%, followed by type 5 (PI) with N=7, p=6\%.
For an example of type 6 error, see \figref{6971}.










\subsection{Second test} \label{sec:results.2}

In \figref{eR_all} can be seen, that the calculated einstein radius $\Theta_E$ of the models tend to be too high.
The overshoot varies from around 0.2 to 0.4 for for good models.
One of the reasons for this is, that it's hard to get the center of the lens on spot.
An offset leads to a a flatter mass profile for the model compared to the simulation.



\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.80\textwidth]{fig/sims/eR_4.png}
  \caption{relative Einstein Radius for each model, per sim. }
  \label{fig:eR_all}
\end{figure}




\hr

Figure \ref{fig:6941} shows a simple example.

Figure \ref{fig:6975} shows an example where substructure introduces a
complication, but model ok.

Figure \ref{fig:6937} shows a case where substructure leads to a
poorer model.

Figure \ref{fig:6975} shows a nice symmetric quad.

Figure \ref{fig:6990} shows a long-axis quad.

Figure \ref{fig:6919} shows a short-axis quad.

Figure \ref{fig:6915} shows an inclined quad.

Figure \ref{fig:6971} shows incorrect identification, but the enclosed
mass still quite well recovered.




\clearpage
